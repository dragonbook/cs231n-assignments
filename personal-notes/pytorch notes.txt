1. retain
* 开始建图：只在tensor的require_grad=True时开始建立（可用于反向传播）的计算图，即设置grad_fn
  例如，输入的预处理部分，不需要grad，所以不会建立连续的计算图（都是叶子节点）。而当有require_grad=True的变量（例如w)
  参与时，会建立计算图，如y = x*w。会有y.grad=AddBackxxxx。

* 建立计算图时：内部node（非leaf node）都有require_grad=True。因为前驱节点至少有一个要求了require grad。
  并且，不能修改内部node的require_grad属性。即tensor.require_grad=False是不允许的，因为有前驱要求。

* 建立计算图时：如果要修改tensor.require_grad，应该是在需要建立一个不需要grad的tensor需求下，那么
  可以使用tensor.detach方法，得到一个共享数据，require_grad=False的叶子节点。
  图中的叶子节点，可以直接设置required_grad，在方向传播时，就不计算这部分的grad，但计算图不会改变。

* 可以使用retain或者hock去获取内部节点的grad以及修改。

* 反向传播时，所有跟最后loss有关联的node都会计算grad。

* 跟.clone, .no_grad等进行区别

* 注意Pytorch是动态图

